\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{tikz-cd}
\usepackage{hyperref}
\renewcommand*{\d}{\mathrm{d}}

\begin{document}

\title{Math 246A HW 2}
\author{Nathan Solomon}
\maketitle

\textbf{Notes 1, Exercises 5, 9, 14, 17; Stein-Shakarchi Chapter 1, Exercises 13, 18. Due Friday, October 13th}

\section{Exercises from Notes 1}

\noindent\fbox{\fbox{\parbox{6.5in}{
            \textbf{5 (Gauss-Lucas Theorem).} Let $P(z)$ be a complex polynomial that is factored as
            \[ P(z)=c(z-z_1)\dots(z-z_n) \]
            for some non-zero constant $c \in \mathbb{C}$ and roots $z_1,\dots,z_n\in \mathbb{C}$ (not necessarily distinct) with $n \geq 1$.
            \begin{itemize}
                \item (i) Suppose that $z_1,\dots,z_n$ all lie in the upper half-plane $\{ z\in\mathbb{C}:\operatorname{Im}(z)\geq0\}$. Show that any root of the derivative $P'(z)$ also lies in the upper half-plane. (\textit{Hint:} use the product rule to decompose the \textit{log-derivative} $ \frac{P'(z)}{P(z)} $ into partial fractions, and then investigate the sign of the imaginary part of this log-derivative for $z$ outside the upper half-plane.)
                \item (ii) Show that all the roots of $P'$ lie in the convex hull of the set $z_1,\dots,z_n$ of roots of $P$, that is to say the smallest convex polygon that contains $z_1,\dots,z_n$.
            \end{itemize}
}}}\bigskip

\begin{itemize}
    \item (i) According to the product rule, the derivative of $P(z)$ is
        \[ P'(z) = c \sum_{i=1}^n \left( \prod_{j \in \{1,2,\dots n\} \backslash \{ i \}} (z - z_j) \right) \]
        Therefore the log-derivative is
        \[ \frac{P'(z)}{P(z)} = \frac{1}{z-z_1} + \frac{1}{z-z_2} + \cdots + \frac{1}{z-z_n} \]
        Outside of the upper half-plane, the imaginary component of $z$ is negative, and since $\operatorname{Im}(z_i) \geq 0$ that means $\operatorname{Im}(z-z_i) < 0$. The reciprocal of $z-z_i$ is a positive multiple of the conjugate of $z-z_i$, so we can conclude that
        \[ \operatorname{Im} \left( \frac{1}{z-z_i} \right) > 0 \]
        The log-derivative of $P(z)$ is the sum of finitely many terms which all have a positive imaginary component, so the log-derivative of $P(z)$ has a positive imaginary component whenever $z$ is outside the upper half-plane.
        \par
        Now suppose $z$ is a root of $P'$ and $z$ is outside the upper half-plane. This cannot be true, since we just proved that if $z$ is outside of the upper half-plane then $P'(z)/P(z)$ has a positive imaginary component, but if $z$ is a root of $P'$, the log-derivative must be zero. Since that's a contradiction, any root of $P'$ must be in the upper half-plane.
    \item (ii) According to a lemma shared in Reid Johnson's office hours, the complex hull $\Omega$ of a finite set of complex numbers is closed and convex, and therefore satisfies the property the following property:
        \par
        For any $z \in \mathbb{C} \backslash \Omega$, there exists a complex number $c$ and a real number $\theta$ such that
            \[ z \in \{ c+e^{i\theta}t : \operatorname{Im}(t) < 0 \} \]
            and
            \[ \Omega \subset \{ c+e^{i\theta}t : \operatorname{Im}(t) \geq 0. \} \]
        \par
        Suppose $\Omega$ is the complex hull of the roots of $P$ and $z_0$ is a root of $P'$ which lies outside $\Omega$. Let $c$ and $\theta$ be constants which satisfy the property above.
        \par
        For each root $z_i$ of $P$, $e^{-i\theta}(z_i - c)$ must be in the upper half-plane, but $e^{-i\theta}(z_0 - c)$ is not in the upper half-plane. However, this implies the polynomial $P(c + e^{i\theta}z)$ has roots only in the upper half plane. By the chain rule, the derivative of that polynomial is $e^{i\theta}P'(c+e^{i\theta}z)$, which has at least one root (namely, $e^{-i\theta}(z_0 - c)$) outside the upper half-plane. That contradicts the result from part (i) though, so we have proven that every root of $P'$ lies in the convex hull of the roots of $P$.
\end{itemize}

\noindent\fbox{\fbox{\parbox{6.5in}{
            \textbf{9 (Ratio test).} If $\sum_{n=0}^\infty a_n (z-z_0)^n$ is a formal power series with the $a_n$ non-zero for all sufficiently large $n$, show that the radius of convergence $R$ of the series obeys the lower bound
            \begin{equation}\label{R-bounds}
                \limsup_{n \rightarrow \infty} \frac{|a_n|}{|a_{n+1}|} \geq R \geq \liminf_{n \rightarrow \infty} \frac{|a_n|}{|a_{n+1}|}
            \end{equation}
            In particular, if the limit $\lim_{n \rightarrow \infty} \frac{|a_n|}{|a_{n+1}|} $ exists, then it is equal to $R$. Give examples to show that strict inequality can hold in either of the bounds in \eqref{R-bounds}. (For an extra challenge, provide an example where \textit{both} bounds are simultaneously strict.)
}}}\bigskip

Suppose $\limsup_{n \rightarrow \infty} |a_n|/|a_{n+1}| < R$. Then there exists a natural number $N$ such that for any $n \geq N$, the ratio $|a_n|/|a_{n+1}|$ is strictly less than $R$. That implies that for any $n \geq N$,
\[ |a_n| > R^{N-n}|a_N|. \]
Since both sides of that inequality are positive, we also know that
\[ \frac{1}{\sqrt[n]{|a_n|}} < \frac{1}{\sqrt[n]{R^{N-n}|a_N|}}. \]
This is useful because the radius of convergence $R$ has been defined in Notes 1, equation (3), as
\[ R := \liminf_{n \rightarrow \infty} \frac{1}{\sqrt[n]{|a_n|}}. \]
The bound we found above is true for any $n \geq N$, but for the sake of comparing limit inferiors, we can ignore a finite number of terms at the beginning, so we have the inequality:
\[ R < \liminf_{n \rightarrow \infty} \frac{1}{\sqrt[n]{R^{N-n}|a_N|}}. \]
The right hand side of that inequality can be rewritten as
\[ \liminf_{n \rightarrow \infty} \left( |a_N|^{-1/n} R^{1-N/n} \right) = \liminf_{n \rightarrow \infty} \frac{R}{\sqrt[n]{|a_N|R^N}} = R. \]
Plugging that back in to the last inequality, we get $R<R$, which cannot be true. Therefore we conclude that
\[ \limsup_{n \rightarrow \infty} \frac{|a_n|}{|a_{n+1}|} \geq R. \]
The proof to show that
\[ R \geq \liminf_{n \rightarrow \infty} \frac{|a_n|}{|a_{n+1}|} \]
is exactly the same, except with all inequalities reversed. Of course, if the limit of $|a_n|/|a_{n+1}|$ exists, then it is equal to the limit supremum and the limit inferior, so it must be equal to $R$.
\par
One example (inspired by \url{https://math.stackexchange.com/a/1708611}) of a case where both bounds are simultaneously strict is when
\[ a_n = 2 + (-1)^n \]
because in that case, we have
\begin{align*}
    \limsup_{n \rightarrow \infty} \frac{|a_n|}{|a_{n+1}|} &= 3 \\
    R = \liminf_{n \rightarrow \infty} \frac{1}{\sqrt[n]{|a_n|}} &= 1 \\
    \liminf_{n \rightarrow \infty} \frac{|a_n|}{|a_{n+1}|} &= \frac{1}{3} \\
\end{align*}

\bigskip
\noindent\fbox{\fbox{\parbox{6.5in}{
            \textbf{14.}
            \begin{itemize}
                \item (i) (Summation by parts formula) Let $a_0, a_1, a_2, \dots, a_N$ be a finite sequence of complex numbers, and let $A_n := a_0 + \cdots + a_n$ be the partial sums for $n=0, \dots, N$. Show that for any complex numbers $b_0, \dots , b_N$ that
                    \[ \sum_{n=0}^N a_nb_n = \sum_{n=0}^{N-1} A_n (b_n - b_{n+1}) + b_N A_N \]
                \item (ii) Let $a_0, a_1, \dots$ be a sequence of complex numbers such that $\sum_{n=0}^\infty a_n$ is convergent (not necessarily absolutely) to zero. Show that for any $0 < r < 1$, the series $\sum_{n=0}^\infty a_n r^n$ is absolutely convergnt, and
                \[ \lim_{r \rightarrow 1^-} \sum_{n=0}^\infty a_n r^n = 0 \]
                (\textit{Hint:} use summation by parts and a limiting argument to express $\sum_{n=0}^\infty a_n r^n$ in terms of the partial sums $A_n = a_0 + \cdots + a_n$.)
            \item (iii) (Abel's theorem) Let $F(z) = \sum_{n=0}^\infty a_n (z-z_0)^n$ be a power series with a finite positive radius of convergence $R$, and let $z_1 := z_0 + R \cdot e^{i \theta}$ be a point on the boundary of the disk of convergence at which the series $\sum_{n=0}^\infty a_n (z_1 - z_0)^n$ converges (not necessarily absolutely). Show that $\lim_{r \rightarrow R^-} F(z_0 + r e^{i \theta}) = F(z_1)$. (\textit{Hint:} use various translations and rotations to reduce to the case considered in (ii).)
            \end{itemize}
}}}\bigskip

\begin{itemize}
    \item (i) First, consider the base case ($N=1$):
        \begin{align*}
            \sum_{n=0}^{N-1} A_n (b_n - b_{n+1}) + b_N A_N &= \\
            \sum_{n=0}^{0} A_n (b_n - b_{n+1}) + b_1 A_1 &= \\
            a_0 (b_0 - b_1) + b_1 (a_0 + a_1) &= \\
            a_0 b_0 + a_1 b_1 &= \sum_{n=0}^N a_nb_n
        \end{align*}
        Therefore the statement is true when $N=1$. If it is true for some natural number $N$, then
        \begin{align*}
            \sum_{n=0}^{(N+1)-1} A_n (b_n - b_{n+1}) + b_{N+1} A_{N+1} &= \\
            \sum_{n=0}^{N} A_n (b_n - b_{n+1}) + b_{N+1} (A_N + a_{N+1}) &= \\
            \sum_{n=0}^{N} A_n b_n + b_{N+1} a_{N+1} &= \\
            \left( \sum_{n=0}^{N-1} A_n b_n + b_N a_N \right) + b_{N+1} a_{N+1} &= \\
            \sum_{n=0}^{N} a_n b_n + b_{N+1} a_{N+1} &= \sum_{n=0}^{N+1} a_n b_n
        \end{align*}
        meaning the statement is true for $N+1$ as well. By induction, the statement must be true for any natural number $N$.
    \item (ii)
    \item (iii)
\end{itemize}

\noindent\fbox{\fbox{\parbox{6.5in}{
            \textbf{17 (Taylor expansion and uniqueness of power series).} Let $F(z) = \sum_{n=0}^\infty a_n(z-z_0)^n$ be a power series with a positive radius of convergence. Show that $a_n = \frac{1}{n!} F^{(n)}(z_0)$, where $F^{(n)}$ denotes the $n^{th}$ complex derivative of $F$. In particular, if $G(z) = \sum_{n=0}^\infty b_n(z-z_0)^n$ is another power series around $z_0$ with a positive radius of convergence which agrees with $F$ on some neighborhood $U$ of $z_0$ (thus, $F(z)=G(z)$ for all $z \in U$), show that the coefficients of $F$ and $G$ are identical, that is to say that $a_n=b_n$ for all $n \geq 0$.
}}}\bigskip

According to theorem 15 from Notes 1, the derivative of $F(z)$ is another power series with the same radius of convergence:
\[ F'(z) = \sum_{k=0}^\infty (k+1) a_{k+1} (z-z_0)^k \]
Repeating that step $n$ times, we get
\[ F^{(n)}(z) = \sum_{k=0}^\infty \left[ (k+1) (k+2) \cdots (k+n) \right] a_{k+n} (z-z_0)^k \]
on the same disc of convergence. To evaluate $F^{(n)}(z_0)$, we ignore all terms except $k=0$, since they are multiplied by $(z-z_0)^k$ and are therefore zero. The result is
\[ F^{(n)}(z_0) = n! a_n \rightarrow a_n = \frac{F^{(n)}(z_0)}{n!} \]
Assume $U$ is open, or remove the limit points so it becomes open, and suppose $F(z) = G(z)$ whenever $z \in U$. The restriction of $F$ to $U$ is exactly the same as the restriction of $G$ to $U$. Also, $U$ is an open set containing $z_0$, and $U$ is fully contained in the radius of convergence of $F$. Since the derivative of $F|_U(z)=G|_U(z)$ is well defined, the following statements are all true:
\begin{align*}
    F(z_0) &= G(z_0) \\
    F'(z_0) &= G'(z_0) \\
    \cdots &= \cdots \\
    \forall n \in \mathbb{N}, F^{(n)}(z_0) &= G^{(n)}(z_0) \\
\end{align*}
Using the result from the first half of this problem (and the fact that factorials are nonzero), that implies $a_n=b_n$ for any nonnegative integer $n$.

\section{Exercises from Stein \& Shakarchi Chapter 1}

\noindent\fbox{\fbox{\parbox{6.5in}{
            \textbf{13.} Suppose that $f$ is holomorphic in an open set $\Omega$. Prove that in any one of the following cases:
            \begin{itemize}
                \item (a) $\operatorname{Re}(f)$ is constant;
                \item (b) $\operatorname{Im}(f)$ is constant;
                \item (c) $|f|$ is constant;
            \end{itemize}
            one can conclude that $f$ is constant.
}}}\bigskip

For convenience, instead of working with $f : \mathbb{C} \rightarrow \mathbb{C}$, I'll use $u(x, y)$ to mean $\operatorname{Re}(f(x+iy))$ and $v(x, y)$ to mean $\operatorname{Im}(f(x+iy))$.
\par
According to the Cauchy-Reimann equations (Notes 1, equations (10) and (11)), $f$ cannot be holomorphic unless both of the following are true:
\begin{align*}
    \frac{\partial u}{\partial x} &= \frac{\partial v}{\partial y} \\
    \frac{\partial v}{\partial x} &= - \frac{\partial u}{\partial y}
\end{align*}

\begin{itemize}
    \item (a) If the real component of $f$ is constant, then $u$ is constant with respect to $x$ and with respect to $y$, so both sides of both of the above equations are zero. That implies $v$ is also constant with respect to both $x$ and $y$, and since $u$ and $v$ are both constant, $f$ is too.
    \item (b) $f$ is constant for the same reason as in part (a), except instead of using the fact that $u$ is constant to prove $v$ is constant, we use the fact that $v$ is constant to prove $u$ is constant.
    \item (c) If $|f|$ is constant, then so is $u^2+v^2$, meaning
        \begin{align*}
            0 &= \Delta (u^2+v^2) \\
              &= \left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} \right) (u^2+v^2) \\
              &= \frac{\partial}{\partial x} \left( 2u \frac{\partial u}{\partial x} + 2v \frac{\partial v}{\partial x} \right) + \frac{\partial}{\partial y} \left( 2u \frac{\partial u}{\partial y} + 2v \frac{\partial v}{\partial y} \right) \\
              &= 2 \left[ \left( \frac{\partial u}{\partial x} \right)^2 + \left( \frac{\partial v}{\partial x} \right)^2 + \left( \frac{\partial u}{\partial y} \right)^2 + \left( \frac{\partial v}{\partial y} \right)^2 \right] + 2u \left[ \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} \right] + 2v \left[ \frac{\partial^2 v}{\partial x^2} + \frac{\partial^2 v}{\partial y^2} \right]\\
              &= 2 \left[ \left( \frac{\partial u}{\partial x} \right)^2 + \left( \frac{\partial v}{\partial x} \right)^2 + \left( \frac{\partial u}{\partial y} \right)^2 + \left( \frac{\partial v}{\partial y} \right)^2 \right]
        \end{align*}
        That last step is possible because of Laplace's equation (Notes 1, equation (13)), which we can apply to $u$ and $v$ because they're harmonic. From here, it's clear that all those derivatives must be zero, since otherwise, the sum of their squares would be positive. That means both $u$ and $v$ are constant with respect to both $x$ and $y$, therefore $f$ is constant.
\end{itemize}

\noindent\fbox{\fbox{\parbox{6.5in}{
            \textbf{18.} Let $f$ be a power series centered at the origin. Prove that $f$ has a power series expansion around any point in its disc of convergence.
            \par
            [Hint: Write $z=z_0+(z-z_0)$ and use the binomial expansion for $z^n$.]
}}}\bigskip

Let $a_n$ be the coefficient of $z^n$ in $f(z)$, so
\[ f(z) = \sum_{n=0}^\infty a_nz^n \]
Also let $R$ be the radius of convergence of $f$ and let $z_0$ be any complex number with magnitude less than $R$ -- that is, $z_0$ is in the disc of convergence of $f$.
\par
Now we can rewrite $f(z)$ in terms of a binomial expansion, and guess that swapping the order of summations is valid (we will prove this shortly):
\begin{align*}
    f(z) &= \sum_{n=0}^\infty a_n(z_0+(z-z_0))^n \\
         &= \sum_{n=0}^\infty \sum_{k=0}^n a_n \binom{n}{k} z_0^{n-k} (z-z_0)^k \\
         &= \sum_{k=0}^\infty \left[ \sum_{n=k}^\infty a_n \binom{n}{k} z_0^{n-k} \right] (z-z_0)^k.
\end{align*}
According to \url{https://qr.ae/pKtSXO}, swapping the order of the summations is allowed if it converges absolutely -- that is, if
\[ \sum_{n=0}^\infty \sum_{k=0}^n \left| a_n \binom{n}{k} z_0^{n-k} (z-z_0)^k \right| < \infty. \]
Alas, I can't figure out how to prove this.

\end{document}
