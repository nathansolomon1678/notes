\documentclass{article}
\input{../preamble}

\fancyhf{}
\setlength{\headheight}{24pt}

\date{\today}
\title{Math 151A Lecture Notes, Fall 2024}

\begin{document}
\maketitle

\tableofcontents

\section{Lecure 1}
$f \in C([a, b])$ means that $f$ is continuous on $[a,b]$. $f \in C^n([a,b])$ means that $f$ is $n$ times continuously differentiable (that is, $f^{(n)}$ is continuous) on $[a, b]$.
\par
Intermediate Value Theorem (IVT) says that if $f \in C([a,b])$, and there exists $k \in \R$ such that either $f(a) \leq k \leq f(b)$ or $f(b) \leq k \leq f(a)$, then there exists at least one $c \in [a,b]$ such that $f(c)=k$. If $f$ is strictly increasing or strictly decreasing on $[a,b]$, then $c$ is unique.
\par
Taylor's theorem says that if $f \in C^n([a,b])$ and $x_0 \in [a,b]$ and $f^{(n+1)}$ exists (but is not necessarily continuous) on $[a,b]$, then for every $x \in [a,b]$, there exists some $\xi_x \in [x_0, x]$ (or in $[x, x_0]$, which is technically the same) such that $f(x) = P_n(x) + R_n(x)$, where
\begin{align*}
    P_n(x) &= \sum_{k=0}^n \frac{(x-x_0)^k f^{(k)}(x_0)}{k!} \\
    R_n(x) &= \frac{(x-x_0)^{n+1}f^{(n+1)}(\xi_x)}{(n+1)!}.
\end{align*}

\section{Lecure 2}
There are 3 types of error that we need to worry about:
\begin{itemize}
    \item \textbf{Computational error} is any error that occurs because computers can only store a finite number of digits. Rounding, truncation, overflow, and underflow are all types of computational error.
    \item \textbf{Truncation error} is error that occurs when you use algorithms that are designed for approximating. For example, approximating a Taylor polynomial with finitely many terms or approximating an integral with a Riemann sum will result in truncation error.
    \item \textbf{Data error} is error that occurs because of noise or measurement error in data. Such errors can be either random or systematic.
\end{itemize}
The floating point form $Fl(x)$, is the computer's representation of $x$. We can always write $Fl(x) = x + \varepsilon$, where $\varepsilon$ is the rounding error.
\par
When a computer does math, it will either round or chop after each step, so we want to reduce the number of Floating Point Operations (FLOPs). Rewriting an equation in ``nested form" can reduce the number of FLOPs, which makes it faster and (usually) more accurate to compute. For example, calculating $x^3-6.x^2+3.2x+1.5$ requires 8 FLOPs, but $x(x(x-6.1)+3.2)+1.5$ only requires 5 FLOPs.
\par
If $p^*$ is an approximation of $p$, then the absolute error is $\abs{p^*-p}$ and the relative error is $\abs{p^*-p}/\abs{p}$ (assuming $p \neq 0$).

\section{Lecure 3}
We say that $f(x)$ is $O(g(x))$ as $x \rightarrow +\infty$ iff there exist $M, x_0 \in \R$ such that $\abs{f(x)} \leq M \abs{g(x)}$ for any $x > x_0$. In other words, $f(x)$ is asymptotically bounded by (a scalar multiple of) $g(x)$.
\par
We say that $f(x)$ is $O(g(x))$ as $x \rightarrow a$ iff there exist $M, \delta \in \R$ such that $\abs{f(x)} \leq M \abs{g(x)}$ whenever $\abs{x-a}<\delta$.
\par
For practical purposes, you can replace those less-than-or-equal-to signs with equal signs, because it would make sense to call merge sort and FFT ``$O(n^2)$" algorithms when you could instead say they are ``$O(n \log n)$" algorithms.
\par
A sequence $x_n$ is said to converge with order of convergence $\alpha \geq 1$ to $x$ iff $x_n$ converges to $x$ and there exists $L \in (0, \infty)$ such that
\[ \lim_{n \rightarrow \infty} \frac{\abs{x_{n+1}-x}}{\abs{x_n-x}^\alpha} = L.\]
$L$ is called the asymptotic error constant.
\par
If $\alpha=1$ and $L<1$, then we say $x_n$ converges linearly. If $1 < \alpha < 2$, we say it converges super-linearly, and if $\alpha=2$, we say it converges quadratically.

\section{Lecure 4}
Given a function $f \in C([a,b])$ such that $f(a)f(b)<0$, the bisection method will converge linearly to one root of $f$. You start by saying $a_1=1, b_1=b$, then at each step, define either $a_{n+1}$ or $b_{n+1}$ to be $(a_n+b_n)/2$, and leave the other endpoint unchanged. The error after $n$ steps is less than or equal to $(b-1)/2^n$, and the residual is $\abs{f(p_n)}$, where $p_n$ is our approximation of a root $p$

\section{Lecure 5}
A fixed point of a function $f$ is a point $p$ such that $f(p)=p$. Saying that $p$ is a fixed point of $f$ is equivalent to saying $p$ is a root of the function $x \mapsto f(x)-x$.
\par
If $f \in C([a,b])$ and $f(x) \in [a,b]$ for all $x \in [a,b]$, then $f$ has at least one fixed point in $[a,b]$. If those criteria are both true and $f'(x)$ is defined on $(a,b)$ and there exists $k \in (0,1)$ such that $\abs{f'(x)} \leq k$ for all $x \in (a,b)$, then the fixed point is unique.
\par
Fixed point iteration (FPI) tries to approximate a fixed point $p$ of $f$ by taking an initial guess $p_0 \in [a,b]$ and defining $p_n=f(p_{n-1})$. If the four criteria above are all true, then the sequence $p_n$ will converge to the unique fixed point $p$ for any choice of $p_0 \in [a,b]$, and
\[ \abs{p_n-p} \leq k^n \max(p_0-a, b-p_0) \]
for all $n$, and
\[ \abs{p_n-p} \leq \frac{k^n}{1-k} \abs{p_1-p_0} \]
for all $n \geq 1$. This implies that $p_n$ converges (at least) linearly to $p$.

\section{Lecure 6}
Newton and Rhapson's method (often just called Newton's method) converges to a root quadratically, so it can be much better than the bisection method or fixed point iteration (but it can also fail in some cases).
\par
If $f \in C^2([a,b])$ and we want to find some root $p$ of $f$, then we choose some initial guess $p_0$ such that $\abs{p_0 - p}$ is small, then define
\[ p_{n+1} = p_n - \frac{f(p_n)}{f'(p_n)}. \]
Newton's method is a local method, meaning it won't converge unless $p_0$ is sufficiently close to $p$. One way to get around this is to do a few iterations of a global root-finding method (like the bisection method), then switch to Newton's method.
\par
If we don't have a computationally efficient way to calculate $f'(x)$, then instead of using Newton's method, we can use the secant method, which takes two initial guesses $p_0$ and $p_1$, then defines
\[ p_{n+1} = p_n - \frac{f(p_n)}{f'(p_n)} \approx p_n - f(p_n) \cdot \frac{(p_n-p_{n-1})}{f(p_n)-f(p_{n-1})}. \]
Newton's method can be generalized to work for functions $f: \R^n \rightarrow \R^n$, in which case we want to find a vector $p$ such that $f(p)=0$. For this situation, we define
\[ p_{n+1}=p_n - J_f(p_n)^{-1}f(p_n), \]
where $J_f$ is the Jacobian matrix:
\[ \left[ J_f(x) \right]_{i,j} := \frac{\partial f_i(x)}{\partial x_j}. \]

\section{Lecure 7}
Newton's method converges quadratically ($\alpha = 2$) and the secant method converges super-linearly ($\alpha =\phi = (1 + \sqrt{5})/2 \approx 1.618$, which we will not bother to prove).
\par
First, we want a general theorem for proving how fast FPI converges. If $g \in C^\alpha([a,b])$ for some integer $\alpha \geq 2$ and $p \in [a,b]$ is a point such that $g(p)=p$ and $g'(p)=g''(p)=\cdots = g^{(\alpha-1)}(p)=0$, but $g^{(\alpha)} \neq 0$, then the sequence defined by $p_{n+1}=g(p_n)$ converges to $p$ with order of convergence $\alpha$ for all $p_0$ sufficiently close to $p$. To prove this, write the Taylor series expansion for $g$ centered at $p$, evaluate it at $p_n$, and apply Taylor's theorem. You will see that $L = \frac{1}{\alpha!} \lim_{n \rightarrow \infty} \abs{g^{(\alpha)}(\xi_n)}$, where $\xi_n$ is between $p$ and $p_n$. Therefore $L = \frac{1}{\alpha!} \abs{g^{(\alpha)} (p)}$.
\par
If we define
\[ g(x) = x - \frac{f(x)}{f'(x)}, \]
then
\[ g'(x) = x - \frac{f(x)f''(x)}{(f'(x))^2}, \]
which is only defined if $f'(x) \neq 0$. If $f'(x) \neq 0$, then by the above theorem, $p_n$ (defined by $p_{n+1} = g(p_n)$) converges to $p$ with order of convergence $\alpha \geq 2$ for $p_0$ sufficiently close to $p$.

\section{Lecure 8}
A point $p$ such that $f(p)=0$ is called a zero (of $f$) of multiplicity $m$ iff there exists a function $q$ such that $f(x) = (x-p)^m q(x)$ for any $x \neq m$, and $q$ is continuous in a neighborhood of $m$, and $q(p) \neq 0$.
\par
The point $p \in (a,b)$ is a zero of multiplicity $m$ of a function $f \in C^m([a,b])$ iff $0 = f(p) = f'(p) = \cdots = f^{(m-1)}(p)$, but $f^{(m)}(p) \neq 0$.
\par
If $m \geq 1$ and $p$ is a zero of $f$ of multiplicity $m$, then the function $\mu(x) := f(x)/f'(x)$ has a zero of multiplicity 1 at $p$. If we want to find a root $p$ of $f$ using Newton's method, but we know that $p$ is a zero (of $f$) of multiplicity $m > 1$, then we can instead define $p_{n+1} = p_n - \mu(p_n)/\mu'(p_n)$. This should also converge quadratically.
\par
Atkien's Acceleration Theorem says that if $p_n$ converges linearly to $p$, and $(p_{n+1}-p)(p_n-p)>0$ for sufficiently large $n$, then the sequence $\hat{p}_n := p_n - (p_{n+1}-p_n)^2/(p_{n+2}-2p_{n+1}+p_n)$ satisfies $\lim_{n \rightarrow \infty} \abs{\hat{p}_n-p}/\abs{p_n-p} = 0$, meaning $\hat{p}_n$ converges to $p$ faster than $p_n$ does.

\section{Lecure 9}
The Weierstrass approximation theorem (also called the Stone-Weierstrass theorem) says that if $f \in C([a,b])$, then for any $\varepsilon > 0$ there exists a polynomial $P(x)$ such that $\abs{f(x)-P(x)} < \varepsilon$ for all $x \in [a,b]$.
\par
Given $n+1$ data points (each data point is a pair $(x_i, f(x_i))$, and $x_i \neq x_j$ unless $i=j$), the Lagrange polynomial is the unique degree $n$ polynomial which goes through all of those points.
\par
Here is an explicit formula for the Lagrange polynomial of degree $n$:
\[ P(x) := \sum_{k=0}^n f(x_k)L_{n,k}(x), \hspace{1cm} L_{n,k}(x) := \prod_{i \in [0,n]\cap \Z - \{k\}} \frac{x-x_i}{x_k-x_i}. \]
Note that $ \left\{ L_{n,k} \right\}$ is basis for the vector space of polynomials of degree $n$.

\section{Lecure 10}
If we have $ \left\{ x_0, x_1, \dots, x_n \right\} \subset [a,b]$ and $x_0 < x_1 < \cdots < x_n$, and $f \in C^{n+1}([a,b])$, and $P(x)$ is the $n$th degree Lagrange polynomial for $f$, then for all $x \in [a,b]$, there exists $\xi(x) \in [a,b]$ such that
\[ f(x) = P(x) + R(x), R(x) := \frac{f^{(n+1)}(\xi(x))}{(n+1)!}(x-x_0)(x-x_1)\cdots (x-x_n). \]

\section{Lecure 11}
Neville's method

\section{Lecure 12}

\section{Lecure 13}

\section{Lecure 14}

\section{Lecure 15}

\section{Lecure 16}

\end{document}
