\documentclass{article}
\input{../preamble}

\fancyhf{}
\setlength{\headheight}{24pt}

\date{\today}
\title{Physics 245 Lecture Notes, Fall 2024}

\begin{document}
\maketitle
\fig{quantum_computer_photo.jpg}

\tableofcontents

\section{9/26/2024 lecture}
\noindent\fbox{\fbox{\parbox{6.5in}{
            \begin{itemize}
                \item Course professor: Eric Hudson
                \item Grade is either (50\% homework, 25\% midterm, and 25\% final), or (50\% homework, 10\% midterm, and 40\% final).
                \item The midterm and final will ask questions from discussion section. ALso, the midterm will be during discussion section (on November 8th).
                \item Homeworks were going to be due every Tuesday before class (10:30 am), but this has been changed to Thursdays.
    \end{itemize}
}}}\bigskip\par
All information is stored in physical systems. For example, a book stores information via the arrangement of ink on paper, and a hard drive stores memory by magnetizing regions of some material. \textbf{Since information is physical, information processing is govered by physics.} The goal of quantum computing is to take full advantage of that, by manipulating information in ways that is not possible with a classical computer.
\bigskip
\begin{thm}
    The Hamiltonian operator on a quantum wavefunction $\Psi(x, t)$ is defined as
    \[ H := \left( V(x, t) - \frac{\hbar^2}{2m} \nabla^2 \right). \]
    The Schrödinger equation says that the eigenvalues of $H$ are energy values, and that
    \[ H\Psi(x, t) = i \hbar \frac{\partial}{\partial t} \Psi(x, t). \]
\end{thm}
\begin{example}
    The ``infinite square well" is a 1-dimensional system in which a particle is trapped in a potential
    \[ V(x) := \begin{cases}
        0 & x \in [0, L] \\
        +\infty & x \not\in [0, L]
    \end{cases}. \]
    Any solution to the Schrödinger equation is a linear combination of basis states $\Psi_n(x, t), n \in \N$:
    \[ \Psi_n(x, t) = \sqrt{ \frac{2}{L}} \sin \left( \frac{n \pi x}{L} \right) e^{-i E_n t / \hbar}. \]
    where the energy of the $n$th state is
    \[ E_n = \frac{\hbar^2 n^2 \pi^2}{2 m L^2}. \]
\end{example}
The infinite square well is pretty much the simplest example of a situation where we can solve the Schrödinger equation. For a slightly more interesting version of that, want to see what happens when we start with a wavefunction of the form
\[ \Psi(x,t) = \sum_{n \in \N} a_n(t) \Psi_n(x, t), a_n \in \C \]
and then suddenly turn on a potential
\[ V(x) = \begin{cases}
    \Omega \cos (\pi x /L) & x \in [0, L] \\
    +\infty & x \not\in [0, L]
\end{cases}. \]
Plugging in our wavefunction and Hamiltonian to the Schrödinger equation, we get
\begin{align*}
    H(x, t) \sum_{n \in \N} a_n \Psi_n(x, 0) e^{-i E_n t / \hbar} &= i \hbar \frac{\partial}{\partial t} \left( \sum_{n \in \N} a_n \Psi_n(x, 0) e^{-i E_n t / \hbar} \right) \\
\sum_{n \in \N} a_n \left( \Omega \cos \left( \frac{\pi x}{L} \right) + E_n \right) \Psi_n(x, 0) e^{-i E_n t / \hbar} &= i \hbar \sum_{n \in \N} \left( \dot{a}_n e^{-i E_n t / \hbar} - \frac{iE_na_n}{\hbar} e^{-iE_nt/\hbar} \right) \Psi_n(x, 0) \\
\Omega \cos \left( \frac{\pi x}{L} \right) \sum_{n \in \N} a_n \Psi_n(x, 0) e^{-iE_nt/\hbar} &= i \hbar \sum_{n \in \N} \dot{a}_n \Psi_n(x, 0) e^{-iE_nt/\hbar} \\
    \frac{\Omega}{2} \sum_{n \in \N} a_n \left( \Psi_{n-1}(x, 0) e^{-iE_nt/\hbar} + \Psi_{n+1}(x, 0) e^{-iE_nt/\hbar} \right) &= i \hbar \sum_{n \in \N} \dot{a}_n \Psi_n(x, 0) e^{-iE_nt/\hbar}.
\end{align*}
If you continue working through this math, and you let $a_{-1}=0$ and let $\omega_m = (E_{m+1}-E_m)/\hbar$, then you get that for any $m \in \N$,
\[ i \hbar \dot{a}_m = \frac{\Omega}{2} \left( a_{m+1} e^{-i\omega_m t} + a_{m-1} e^{i\omega_{m-1}t} \right).  \]
Now suppose we make the potential time-dependent (with frequency $a_1$), so on $x \in [0,L]$, we have
\[ V(x, t) = \Omega \cos(\omega_1 t) \cos \left( \frac{\pi x}{L} \right), \]
which will give us a different equation for each $\dot{a}_n$, the first two of which are
\begin{align*}
    i\hbar \dot{a}_1 &= \frac{\Omega}{2} a_2 \frac{1+e^{-2i\omega_1t}}{2} \\
    i\hbar \dot{a}_2 &= \frac{\Omega}{2} \left( a_3 e^{-i\omega_2t} \left( \frac{e^{i\omega_1t}-e^{-2i\omega_1t}}{2}\right) + a_1 \left( \frac{1+e^{2i\omega_1t}}{2} \right) \right).
\end{align*}
This looks messy to solve, but we can make use the Rotating Wave Approximation (RWA) to ignore the messy parts. See my solution to homework 1 for an explanation of the RWA.
\par
After applying the RWA, we just have two equations, which can be written as
\[ \frac{\partial}{\partial t} \begin{bmatrix}
    a_1 \\
    a_2
\end{bmatrix} = \frac{\Omega}{4i\hbar} \begin{bmatrix}
0 & 1 \\
1 & 0
\end{bmatrix} \begin{bmatrix}
    a_1 \\
    a_2
\end{bmatrix}. \]
One solution to this is $a_2 = \sin(\Omega t / 4 \hbar)$, and $a_2 = i\cos(\Omega t / 4 \hbar)$.

\section{10/1/2024 lecture}
An ideal qubit is a perfectly isolated 2-level system, but in reality, the 2 states will always be coupled to states of an external system. For example, if our qubit is an electron in a magnetic field whose available states are spin-up and spin-down, then any electric field nearby will cause Stark shifting of the energy levels. Also, unless we can force the electron to be in the ground state, its own spatial wavefunction will create electromagnetic effects that interact with its spin.
\bigskip
\par
A vector space $V$ over a field $\F$ is a non-empty set $V$ with vector addition ($+ : V \times V \rightarrow V$) and scalar multiplication $\cdot : \F \times V \rightarrow V$) such that for any $u,v,w \in V$ and any $a,b \in \F$,
\begin{itemize}[noitemsep]
    \item $u+(v+w)=(u+v)+w$
    \item $u+v=v+u$
    \item There exists $0 \in V$ such that $v+0=v$
    \item There exists $(-v) \in V$ such that $v+(-v)=0$
    \item $a(bv)=(ab)v$
    \item If $1 \in \F$ is the multiplicative identity, then $1v=v$
    \item $a(u+v)=au+av$
    \item $(a+b)v=av+bv$
\end{itemize}
A  normed vector space $V$ over some field $\F$ also has a norm $\norm{\cdot} : V \rightarrow \R$ such that for any $u,v \in V, a \in \F$,
\begin{itemize}[noitemsep]
    \item $V$ satisfies the axioms of a vector space over $\F$
    \item $\norm{u} \geq 0$, and $\norm{u}=0$ iff $u=0$
    \item $\F$ is a subfield of $\C$, and $\norm{au}=\abs{a} \cdot \norm{u}$. Technically, there may be some other fields that would work, like the $p$-adic numbers, but we don't care about those. In fact, some definitions of a normed vector space would require that $\F$ is either $\R$ or $\C$
    \item $\norm{x+y}\leq \norm{x}+\norm{y}$
\end{itemize}
Note that a normed vector space is also a metric space, because if we define $d: V \times V \rightarrow V$ by $d(u,v)=\norm{u-v}$, then $(V, d)$ satisfies the metric space axioms. We call $V$ a Banach space $V$ iff it is complete in that metric.
\par
An inner product space $X$ is a vector space over $\F$, where $\F$ is either $\R$ or $\C$, with an inner product $\mean{\cdot, \cdot}: X \times X \rightarrow X$ such that for any $x,y,z \in X, a,b \in \F$,
\begin{itemize}[noitemsep]
    \item $\mean{y,x} = \overline{\mean{x, y}}$
    \item $\mean{ax+by, z} = a \mean{x,z}+b\mean{y,z}$
    \item $\mean{x, x} \geq 0$, and $\mean{x,x}=0$ iff $x=0$
\end{itemize}
\begin{note}
    Mathematicians define the inner product to be linear in the first argument ($\mean{ax,y}=a\mean{x,y}$) and conjugate-linear in the second argument ($\mean{x,ay}=\overline{a}\mean{x,y}$). However, in Dirac's bra-ket notation, which physicsts use, the inner product is conjugate linear in the first argument ($ \braket{ax}{y} = a^* \braket{x}{y}$) and linear in the second argument ($ \braket{x}{ay} =a \braket{x}{y}$). We can clarify this by using different notation, and by assuming that if you are reading about quantum mechanics, you will see the physicists' notation. But if there is any ambiguity, specify which you are using.
\end{note}
\par
We call $V$ a Hilbert space iff it is a Banach space, and it is an inner product space, and $\norm{x}^2=\mean{x,x}$ for any $x \in V$. Any quantum wavefunction, inluding spatial wavefunctions, spin wavefunctions, and multiple-particle wavefunctions is an element of a complex Hilbert space.
\begin{thm}
    ``A Hilbert space is a safe space -- it's where physicists like to be" -- prof. Hudson.
\end{thm}
Whenever we are working with a Hilbert space, we take for granted that there is a countable orthonormal basis, which we often index by nonnegative integers.
\begin{thm}
    Carleson's theorem: If $f$ is a periodic $L^2$ function (that is, a periodic function whose magnitude squared can be integrated over one period), its Fourier series converges to $f$ ``almost everywhere", meaning the set of points where the Fourier series for $f$ doesn't coverge to $f$ has measure zero.
\end{thm}
In bra-ket notation, the ket $\ket{\psi}$ is a vector in a Hilbert space $\mathcal{H}$, which represents some sort of quantum wavefunction. Complex conjugation is a bijection $^* : \C \rightarrow \C$ which maps $a+bi$ to $a-bi$, where $a,b \in \R$. The conjugate transpose (also called adjoint, dagger, Hermitian conjugate, or dual) of a ket is called a bra, and is written as $\ket{\psi}^\dag=\bra{\psi}$. A bra is an element of the dual space $\mathcal{H}^*$, which is naturally ``isometrically anti-isomorphic" to $\mathcal{H}$, according to the Riesz representation theorem. However, some mathematicians might be upset with me for saying a bra is the dual space, because it could lead to some confusion about whether the vectors transform covariantly or contravariantly.
\par
A matrix $A$ is called Hermitian iff $A=A^\dag$, and unitary iff $A^{-1}=A^\dag$.
\begin{cor}
    Being real and symmetric is the same as being real and Hermitian. Also, being real and orthogonal (aka orthonormal) is the same as being real and unitary.
\end{cor}

\section{10/3/2024 lecture}
\begin{thm}
    For any Hermitian operator $A$, there is an orthonormal basis of eigenvectors of $A$, and all eigenvalues of $A$ are real.
\end{thm}
\begin{proof}
    This is an exercise for the reader. Note that in the case of degenerate eigenvalues, we can already assume each eigenspace has an orthonormal basis, so we just need to show that elements of different eigenspaces are orthogonal, then use the axiom of choice.
\end{proof}
The postulates of quantum mechanics say that any obesrvable operator $A$ (including the Hamiltonian) will always be Hermitian, so $A=\sum \lambda_k \ket{v_k}\bra{v_k}$ for some normalized eigenvectors $\ket{v_k}$ and corresponding eigenvalues $\lambda_k$. Furthermore, if $\ket{\psi} \in \mathcal{H}$ is normalized, then when you ``measure $A$", then for each $k$ there is probability $\abs{\braket{\psi}{v_k}}^2$ that the wavefunction will instantly collapse into state $\ket{v_k}$ and the value you measure will be $\lambda_k$. Therefore the expected value of $A$ is
\[ \mean{A} = \sum_k \lambda_k \abs{\braket{\psi}{v_k}}^2. \]
The standard deviation in measured values (assuming we can somehow reset the state to $\ket{\psi}$ after each measurement) is
\[ \sigma_A := \sqrt{\mean{A^2}-\mean{A}^2}. \]


\section{10/8/2024 discussion/review}
If the Hamiltonian does not depend on time, then the time evolution operator $U(t, t_0)$ can be defined as
\[ U(t, t_0) := \exp \left( - \frac{i (t - t_0) H}{\hbar} \right), \]
and it is useful because
\[ \ket{\Psi(t)} = U(t, t_0) \ket{\Psi(t_0)}. \]
If $H$ is diagonal then $U$ is as well.
\par
\begin{lem}
    The exponential of a skew-Hermitian matrix is unitary, and the determinant of the exponential of a traceless matrix is one.
\end{lem}
\begin{proof}
    If $A$ is skew-Hermitian, then $-A=A^\dag$, so
    \[ \exp(A)^{-1}=\exp(-A)=\exp(A^\dag)=\exp(A)^\dag, \]
    therefore $\exp(A)$ is unitary.
    \par
    If $A$ is traceless, then
    \[ \det(\exp(A)) = \exp(\Tr(A)) = \exp(0) = 1. \]
\end{proof}
Since $H$ is an $n \times n$ Hermitian matrix ($n$ could be infinity), then $i H t / \hbar \in \mathfrak{u}(n)$ is skew-Hermitian (also called anti-Hermitian, meaning its adjoint is its negative). Therefore its exponential, $\exp(iHt/\hbar) \in U(n) = (SU(n) \oplus U(1)) / \Z_n$, is unitary. By going into the interaction picture, we could also ensure $H$ is traceless, so $i H t /\hbar \in \mathfrak{su}(n)$ is traceless, so $\exp(iHt/\hbar) \in SU(n)$.

\section{10/10/2024 lecture}
The energy of a magnetic moment $\mu$ in a magnetic field is $E = - \mu \cdot B$. For most of this class, we will think of a 2-state qubit as an electron in a magnetic field, but in reality, any Two-Level System (TLS) is a qubit, and the physics is the same for any TLS.
\par
The spin state of the qubit can be written as
\[ \ket{\Psi} = c_0 \ket{\uparrow} + c_1 \ket{\downarrow}, \]
which can also be written as
\[ \ket{\Psi} = c_0 \ket{0} + c_1 \ket{1} = \begin{bmatrix}
    c_0 \\
    c_1
\end{bmatrix}, \]
where $c_0, c_1 \in \C$ satisfy $\norm{c_0}^2+\norm{c_1}^2 = 1$.
\begin{note}
    $\ket{0}$ represents the ``up"/``excited" state and $\ket{1}$ is the ``down"/``ground" state. This can be confusing because we are used to indexing states like $\ket{n}$ for the harmonic oscillator, where energy increases as $n$ increases. However, in this case, the $\ket{0}$ state has positive energy and the $\ket{1}$ state has negative energy.
\end{note}
The Hamiltonian for our electron is
\[ H = -\mu \cdot B = \frac{g \mu_B}{\hbar} S \cdot B, \]
where $g = -2.002319 \approx 2$ is the gyromagnetic ratio for the electron and
\[ \mu_B = \frac{e \hbar}{2 m_e} = 9.274 \times 10^{-24} J/T \]
is the ``Bohr magneton".
\par
If $B_x$ and $B_y$ are both zero, then
\[ H = \frac{g \mu_B B_z}{\hbar} S_z = \frac{g \mu_B B_z}{\hbar} \cdot \frac{\hbar \sigma_z}{2} \approx \mu_B B_z \sigma_z = \mu_B B_z \begin{bmatrix}
    1 & 0 \\
    0 & -1
\end{bmatrix}. \]


\section{10/15/2024 lecture}

\section{10/17/2024 lecture}
The \textbf{interaction picture} is an extremely useful tool for simplifying problems. The main idea is that we can factor a unitary matrix out of the time evolution operator, and this is sort of like changing to a moving reference frame.

\section{10/22/2024 lecture}

\section{10/24/2024 lecture}
This lecture was recorded, see Bruinlearn/pages

\section{10/29/2024 lecture}

\section{10/31/2024 lecture}

\section{11/5/2024 lecture}
If we drive a harmonic oscillator with a force $F=-F_0\sin(\omega t + \phi)$, the Hamiltonian is
\[ \frac{H}{\hbar} = \omega (a^\dag a + \frac{1}{2}) + \beta \sin (\omega t + \phi) (a + a^\dag). \]
The corresponding time evolution operator is
\[ D(\alpha) := U = \exp \left( \alpha a^\dag - \alpha^* a \right) \]
where $\alpha \propto \beta t$. We label the coherent states as
\[ \ket{\alpha} = D(\alpha) \ket{0}, \]
but this is in the interaction picture. If we leave the interaction picture, we have
\[ U_0 D(\alpha) \ket{0} = \ket{\alpha e^{-i \omega t}}. \]

\subsection{Physically implementing a QHO}
We have studied how to do quantum gates on a harmonic oscillator, but how do we actually implement a system like that? An LC circuit looks like a harmonic oscillator, where
\[ H = \frac{\Phi}{2L} + \frac{L \omega^2 Q}{2}. \]
We can also create a system of trapped ions by putting one ion or more in a line, then laser cooling them so that they remain on that line (instead of wiggling around).
\fig{LIT - Geometry.png}

\subsection{Composite systems}
Suppose we have 2 qubits,
\begin{align*}
    \ket{\Psi_1} &= a \ket{0} + b \ket{1} \\
    \ket{\Psi_2} &= c \ket{0} + d \ket{1}.
\end{align*}
Then the whole systems can be described by a tensor product,
\begin{align*}
    \ket{\Psi_{total}} &= \ket{\Psi_1} \otimes \ket{\Psi_2} \\
                       &= ac \ket{00} + ad \ket{01} + bc \ket{10} +bd \ket{11} \\
                       &= \begin{bmatrix}
                           ac \\
                           ad \\
                           bc \\
                           bd
                       \end{bmatrix}.
\end{align*}
To use this notation, we need to know how to treat operators on the whole system as matrices. For example,
\[ P_{01}=\ket{01}\bra{01}=P_0\otimes P_1 = \begin{bmatrix}
    1 & 0 \\
    0 & 0
\end{bmatrix} \otimes \begin{bmatrix}
    0 & 0 \\
    0 & 1
\end{bmatrix} = \begin{bmatrix}
    0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0
\end{bmatrix}. \]
In other words, to get the operator on the composite system, we take the Kronecker product of the operator on the first qubit and the operator on the second qubit.
\par
If we have a system of $n$ qubits, the state of the system lives in
\[ \mathcal{H}_{total} = \bigotimes_{i=1}^n \mathcal{H}^{(i)}. \]
The state of the system can be ``disentangled" (factored) as a tensor product of elements of $\mathcal{H}^{(1)}, \mathcal{H}^{(2)}, \dots$ iff the qubits are not entangled. For example, the state
\[ \frac{1}{\sqrt{2}} \left( \ket{00} + \ket{11} \right) \]
is entangled, because if you measure the state of one qubit, you know the other qubit must be in the same state. This cannot be factored as
\[ (a \ket{0} + b \ket{1}) \otimes (c \ket{0} + d \ket{1}) \]
because $ad$ and $bc$ would be zero, which implies either $ac$ or $bd$ is zero.
\section{11/7/2024 lecture}
If you have an atom inside a harmonic oscillator, and the atom can be either excited or unexcited (the excited state, $\ket{0}$, has $\omega_0$ more energy that the ground state, $\ket{1}$), the Hamiltonian for the system can be written as
\[ \frac{H}{\hbar} = \frac{\omega_0}{2} \sigma_z \otimes I + \omega I \otimes \left( N + \frac{1}{2} I \right) + V, \]
where the state of the atom is $\ket{\Psi_1}$, the state of the harmonic oscillator that the atom is trapped in (which has frequency $\omega$) is $\ket{\Psi_2}$, and the composite state is $\ket{\Psi_1} \otimes \ket{\Psi_2}$.
\par
The $V$ term is
\[ V := \frac{g}{2} \sigma_x \left( a^\dag + a \right) = \frac{g}{2} \left( \sigma_- + \sigma_+ \right) \left( a^\dag + a \right). \]
This term represets coupling between the atom and the harmonic oscillator. The motivation for this is that if the detuning $\delta = \omega - \omega_0$ is small, then the atom and the harmonic oscillator can exchange enrgy. I still don't understand exactly why we include it.
\par
To go to the interaction picture, we can define $H_0 = H - \hbar V$ and $U_0 = \exp \left( -i H_0 t/\hbar \right) $, so the Hamiltonian in the interaction picture is
\begin{align*}
    H_I &= U_0^\dag V U_0 \\
        &= \frac{g}{2} \left( e^{i \omega_0 \sigma_z t / 2} \sigma_x e^{-i \omega_0 \sigma_z t / 2} \right) \left( e^{i \omega (a^\dag a + 1/2)t} (a+a^\dag) e^{- i \omega (a^\dag a + 1/2)t} \right) \\
        &= \frac{g}{2} \left( e^{i \omega_0 t} \sigma_+ + e^{-i \omega_0 t} \sigma_- \right) \left( e^{-i \omega t} a + e^{i \omega t} a^\dag \right) \\
        &= \frac{g}{2} \left( e^{i (\omega_0-\omega) t} \sigma_+ \otimes a + e^{i (\omega_0+\omega) t} \sigma_+ \otimes a^\dag + e^{i (-\omega_0-\omega) t} \sigma_- \otimes a + e^{i (-\omega_0+\omega) t} \sigma_- \otimes a^\dag \right) \\
        &\approx \frac{g}{2} \left( e^{i (\omega_0-\omega) t} \sigma_+ \otimes a + e^{i (-\omega_0+\omega) t} \sigma_- \otimes a^\dag \right).
\end{align*}
That last step used the rotating wave approximation (RWA).
\par
Now if we go back from the interaction picture to the lab frame,
\[ H = H_0 + \frac{g\hbar}{2} \left( \sigma_+ a + \sigma_- a^\dag \right).  \]
It makes sense that we were able to ignore the terms $\sigma_+ \otimes a^\dag$ and $\sigma_- \otimes a$, because those operators represent the atom becoming excited while the harmonic oscillator gains energy, and the atom becoming unexcited while the harmonic oscillator loses energy, respectively. The whole point of the $V$ term was to represent the atom and the oscillator exchanging energy, but since energy is conserved, neither of those operators are meaningful.
\par
This new Hamiltonian is very important, so we call it the Jaynes-Cummings Hamiltonian:
\[ \frac{H}{\hbar} = \frac{\omega_0}{2} \sigma_z \otimes I + I \otimes \omega \left( N + \frac{1}{2} I \right) + \frac{g}{2} \left( \sigma_+ \otimes a + \sigma_- \otimes a^\dag \right). \]
The eigenstates of this Hamiltonian are denoted $\ket{i,n}$, where $i \in \left\{ 0,1 \right\}$ and $n \in \N$. Note that the coupling term $V$ will only ``link" each state to at most one other state:
\[ \bra{i,m} V \ket{j, n} = \begin{cases}
    \frac{g}{2} \sqrt{n} & i=1, j=0, n=m+1 \\
    \frac{g}{2} \sqrt{m} & i=0, j=1, n=m-1 \\
    0 & \text{otherwise}
\end{cases} \]
Therefore the Hamiltonian can be written in the basis $ \left( \ket{1,0}, \ket{1,0}, \ket{1,1}, \ket{2,0},\dots \right)$ as a block-diagonal matrix, where each block is a symmetric $2 \times 2$ matrix. Note that I left $\ket{0,0}$ out of the basis, because the Hamiltonian is simpler that way -- if we wanted to include it, we would add another entry at the top left (with zeroes below and to the right of it).
\[ H = \begin{bmatrix}
    H^{0} & 0 & 0 & \cdots \\
    0 & H^{1} & 0 & \cdots \\
    0 & 0 & H^{2} & \cdots \\
    \vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}, H^{(n)} = \begin{bmatrix}
    \frac{\omega_0}{2} + \omega \left( n + \frac{1}{2} \right) & \frac{g}{2} \sqrt{n+1} \\
    \frac{g}{2} \sqrt{n+1} & - \frac{\omega_0}{2} + \omega \left( n + \frac{3}{2} \right)
\end{bmatrix} \]
I need to double check all my formulas for these lecture notes, then get notes for the remainder of the lecture, where we solved for the eigenvectors and eigenvalues of each block matrix in order to find the new energy levels (as a function of the detuning $\delta$ and the coupling strength $g$). Also get notes for relating this to the Stark effect.

\section{12/3/2024 lecture}
If we want to predict the time-evolution of a system in an environment, and the environment begins in a state $\ket{e_0}$ (it doesn't need to start in that state, that just makes the math way easier), then
\[ \rho_{SE}(t) = U_{SE}(t) \rho_{SE}(t=0) U_{SE}(t)^\dag, \]
so the state of the system after some time $t$ can be written as
\[ \rho_S(t) = \tr_E \rho_{SE}(t) = \sum_{i \in B} K_i \rho_S(t=0) K_i^\dag \]
where $B = \left\{ e_0, e_1. e_2, \dots \right\}$ is a basis for the environment states, and $K_i$ is the Kraus operator, defined as
\[ K_i := \bra{e_i} U_{SE}(t) \ket{e_0}. \]
Technically, that gives a dimension mismatch error, and Kraus operator should be defined as
\[ K_i := (I_S \otimes \bra{e_i}) U_{SE}(t) (I_S \otimes \ket{e_0}), \]
but we don't bother to write that all out.
\begin{note}
    There are a bunch of different names for the quantum channel description, but they all mean the same thing:
    \begin{itemize}
        \item
    \end{itemize}
\end{note}

\section{12/5/2024 lecture}
If $\rho_{SE}$ is a density matrix for a system in an envirorment, then taking the (partial) trace over the environment will give $\rho_S = \tr_E(\rho_{SE})$, which is the density matrix for just the system. If we want to make some measurement $M_S$ on the system, the ensemble average is $[M_S] =\tr (M_S \rho_S) = \tr \left( (M_S \otimes I_E) \rho_{SE} \right)$.
\par
The time evolution operator for an electron in an optical cavity (and magnetic field) is given by the exponential of the Jaynes-Cummings Hamiltonian (JCH):
\[ U_{JC} = \ket{10}\bra{10} + \cos \frac{\Omega t}{2} \left( \ket{00}\bra{00} + \ket{11}\bra{11} \right) - i\sin \frac{\Omega t}{2} \left( \ket{00}\bra{11} + \ket{11}\bra{00} \right). \]
This assumes the system begins in a superposition of only the $\ket{10}$, $\ket{00}$, and $\ket{11}$ states, because otherwise we would have an infinite-dimensional block-diagonal matrix. If we call the electron the ``system" and we call the QHO the ``environment", then we can compute the Kraus operators (I NEED TO REVIEW HOW TO ACTUALLY COMPUTE THESE BY HAND):
\[ K_j = \begin{cases}
    \ket{1}\bra{1} + \cos \frac{\Omega t}{2} \ket{0}\bra{0} & j = 0 \\
    -i \sin \frac{\Omega t}{2} \ket{1}\bra{0} & j = 1 \\
    0 & j > 1
\end{cases} \]
We can let $p$ (HOW DO WE DEFINE $p$ IN TERMS OF THE RABI FREQUENCY AND TIME???)be the probability of emitting a photon and then make a small-time approximation, to get
\begin{align*}
    K_0 &= \ket{1}\bra{1} + \sqrt{1-p} \ket{0}\bra{0} \\
    K_1 &= \sqrt{p} \ket{1}\bra{1} \\
    i > 1 \Rightarrow K_i &=0
\end{align*}
These new operators will not model unitary evolution. If we use them to predict the density matrix after time $t$, we get
\[ \rho_S(t) = (1-p) \ket{0}\bra{0} + p \ket{1}\bra{1}. \]
We call $K_0$ the ``jump operator", because it represents the qubit jumping from the excited state to the ground state, and we call $K_1$ the ``no-jump operator", because it represents the qubit not having jumped states. You might think that the no-jump operator does nothing, so it should be the identity, but that's not true -- if we see that the qubit has not jumped states after time $t$, that tells us that the qubit was more likely to have been in the ground state to begin with.
\par
The most general way to describe the time evolution of some quantum system is with density matrices $\rho$ and with operators $M_i$ \textbf{which are not necessarily unitary}:
\[ \rho(t) = \sum_i M_i(t) \rho(0) M_i^\dag(t). \]
That equation makes it easy to calculate $\rho(t)$ if you know what $M$ is for one specific $t$, but we may want to calculate $\rho(t)$ as a continuous function of $t$.
\par
We already know that
\[ i\hbar \frac{\partial \rho_{SE}}{\partial t} = \left[ H, \rho_{SE} \right], \]
but that's not practical because we never know the wavefunction of the environment. However, we an rewrite it to get the Lindbladian. ADD NOTES ABOUT LINDBLADIAN
\par
The \textbf{fidelity} $\mathcal{F} := \abs{\braket{\phi}{\psi}}^2$ is a measure of how similar the states $\ket{\phi}$ and $\ket{\psi}$ are. If $\ket{\phi}$ is the state that we want our system to be in, then $\mathcal{F}$ is a measure of how close we are to that goal. This is generally how we measure how good a quantum computer is. The best trapped-ion quantum computers can have fidelity around $0.999999$, but most quantum computers are worse. If our density matrix is $\rho := \ket{\psi}\bra{\psi}$, then $\mathcal{F} = \bra{\phi}\rho\ket{\phi}$.
\begin{note}
    WARNING: some people define the fidelity as $\mathcal{F} = \abs{\braket{\phi}{\psi}}$, but this isn't super common.
\end{note}
The \textbf{von Neumann entropy} of a system is
\[ S(\rho) = \tr \left( \rho \log_2 \rho \right), \]
where $\rho$ is the density matrix. Taking the logarithm of a matrix sucks, so you would want to find some orthonormal basis $v_i, i \in \N$ of eigenvectors of $\rho$ (with respective eigenvalues $\lambda_i$), because then
\[ S(\rho) = \sum_{i \in \N} - \lambda_i \log_2 \lambda_i. \]
For a pure state, this works out to zero. For a completely mixed state made up of $n$ qubits, it works out to $S(\rho)=n$.
\par
The \textbf{entanglement entropy} $S_E$ measures how entangled two systems $A$ and $B$ are by seeing how much the von Neumann entropy increases when you trace out $A$ (or equivalently, when you trace out $B$):
\[ S_E(\rho_{AB}) := S(\rho_A) = S(\tr_B(\rho_{AB})). \]


\section{Deferred measurement principle}

\fig{deferred_measurement.png}
Also talk about no-cloning theorem and no-communication theorem
\section{Magnus expansion}
\section{Rotation operator on Bloch sphere}
\section{Interaction picture}
\section{Baker-Campbell-Hausdorff formula}
\section{Displacement operator}
\[ D(\alpha) = \exp \left( \alpha a^\dag -\alpha^* a \right)  \]
Note that the displacement operator takes the vacuum state to a coherent state, but it does not map every coherent state to another coherent state.
\par
Give definition of displacement operator and prove it's basic properties, like $D(\alpha)^\dag = D(-\alpha)$ and $D^\dag(\alpha)D(\alpha)=I$. Also talk about Kermack-McCrae identities, optical phase space, Wigner quasiprobability distribution, Husimi Q representation, and squeezed coherent states. For problem 3 on HW5, figure out explicit formula for probability as a function of $t_w$. For that, refer to \url{https://physics.stackexchange.com/questions/553225/representation-of-the-displacement-operator-in-number-basis}
\section{Mølmer-Sørenson gate}
discuss physical interpretation
\section{Density matrices}
\[ \rho(t) = U(t) \rho(0) U(t)^\dag \]

that equation comes from
\[ i\hbar \frac{\partial \rho}{\partial t} = [H, \rho] \]


\end{document}
